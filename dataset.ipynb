{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c4589bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "from PIL import Image, ImageDraw, ImageOps\n",
    "\n",
    "class ClimbingHoldDataset(Dataset):\n",
    "    def __init__(self, annotations_dir, images_dir, output_size=(128, 128)):\n",
    "        self.images_dir = images_dir\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406], \n",
    "                std=[0.229, 0.224, 0.225]\n",
    "            ),\n",
    "        ])\n",
    "        self.output_size = output_size\n",
    "        self.holds = []\n",
    "\n",
    "        for json_file in os.listdir(annotations_dir):\n",
    "            if json_file.endswith(\".json\"):\n",
    "                json_path = os.path.join(annotations_dir, json_file)\n",
    "                with open(json_path, 'r') as file:\n",
    "                    data = json.load(file)\n",
    "\n",
    "                images = {img['id']: img['file_name'] for img in data.get('images', [])}\n",
    "                annotations = data.get('annotations', [])\n",
    "                difficulties = {cat['id']: cat['name'] for cat in data.get('categories', [])}\n",
    "\n",
    "                for annotation in annotations:\n",
    "                    file_name = images.get(annotation.get(\"image_id\"))\n",
    "                    hold_data = {\n",
    "                        \"image_id\": file_name,\n",
    "                        \"difficulty\": difficulties.get(annotation.get(\"category_id\")),\n",
    "                        \"type\": annotation[\"attributes\"].get(\"Type\"),\n",
    "                        \"orientation\": annotation[\"attributes\"].get(\"Orientation\"),\n",
    "                        \"bbox\": annotation.get(\"bbox\"),\n",
    "                        \"segmentation\": annotation.get(\"segmentation\"),\n",
    "                    }\n",
    "                    self.holds.append(hold_data)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.holds)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        hold_data = self.holds[idx]\n",
    "        image_id = hold_data[\"image_id\"]\n",
    "        image_path = os.path.join(self.images_dir, image_id)\n",
    "        image = Image.open(image_path)\n",
    "        rotated_image = ImageOps.exif_transpose(image)\n",
    "\n",
    "        bbox = hold_data[\"bbox\"]\n",
    "        x_min, y_min, width, height = bbox\n",
    "        cropped_image = rotated_image.crop((x_min, y_min, x_min + width, y_min + height))\n",
    "        cropped_image = cropped_image.resize(self.output_size)\n",
    "\n",
    "        if self.transform:\n",
    "            cropped_image = self.transform(cropped_image)\n",
    "\n",
    "        return {\n",
    "            \"image\": cropped_image,\n",
    "            \"type\": self._map_type(hold_data[\"type\"]),\n",
    "            \"orientation\": self._map_orientation(hold_data[\"orientation\"]),\n",
    "        }\n",
    "\n",
    "    def _map_type(self, type_label):\n",
    "        types = ['Jug', 'Sloper', 'Crimp', 'Jib', 'Pinch', 'Pocket', 'Edge']\n",
    "        return types.index(type_label) if type_label in types else -1\n",
    "\n",
    "    def _map_orientation(self, orientation_label):\n",
    "        orientations = ['Up', 'Down', 'Side', 'UpAng', 'DownAng']\n",
    "        return orientations.index(orientation_label) if orientation_label in orientations else -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6a343ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 1605\n",
      "Image tensor shape: torch.Size([3, 224, 224])\n",
      "Type index: 0\n",
      "Orientation index: 3\n"
     ]
    }
   ],
   "source": [
    "annotations_dir = \"data/annotations\"\n",
    "images_dir = \"data/images\"\n",
    "\n",
    "dataset = ClimbingHoldDataset(annotations_dir, images_dir)\n",
    "\n",
    "print(\"Dataset size:\", len(dataset))\n",
    "\n",
    "sample = dataset[0]\n",
    "\n",
    "print(\"Image tensor shape:\", sample[\"image\"].shape)\n",
    "print(\"Type index:\", sample[\"type\"])\n",
    "print(\"Orientation index:\", sample[\"orientation\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
