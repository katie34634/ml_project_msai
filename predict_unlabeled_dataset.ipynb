{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a566fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, models\n",
    "from torchvision.models import ResNet50_Weights\n",
    "\n",
    "from create_dataset import ClimbingHoldDataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b94dda07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# Choose device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")  # Apple Silicon\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "IMG_SIZE = 224\n",
    "\n",
    "# Evaluation transform (no heavy augmentations)\n",
    "eval_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225],\n",
    "    ),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f287c007",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'NEW_DATA/annotations'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m annotations_dir = \u001b[33m\"\u001b[39m\u001b[33mNEW_DATA/annotations\u001b[39m\u001b[33m\"\u001b[39m  \u001b[38;5;66;03m# <-- change this\u001b[39;00m\n\u001b[32m      2\u001b[39m images_dir = \u001b[33m\"\u001b[39m\u001b[33mNEW_DATA/images\u001b[39m\u001b[33m\"\u001b[39m            \u001b[38;5;66;03m# <-- change this\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m dataset = \u001b[43mClimbingHoldDataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mannotations_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mannotations_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimages_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimages_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mIMG_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIMG_SIZE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m dataset.transform = eval_transform  \u001b[38;5;66;03m# use eval transform\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTotal samples in NEW dataset:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(dataset))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/ml_project_msai/create_dataset.py:29\u001b[39m, in \u001b[36mClimbingHoldDataset.__init__\u001b[39m\u001b[34m(self, annotations_dir, images_dir, output_size)\u001b[39m\n\u001b[32m     26\u001b[39m \u001b[38;5;28mself\u001b[39m.holds = []\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# Iterate over all JSON files in the annotations directory\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m json_file \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mannotations_dir\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m     30\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m json_file.endswith(\u001b[33m\"\u001b[39m\u001b[33m.json\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     31\u001b[39m         json_path = os.path.join(annotations_dir, json_file)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'NEW_DATA/annotations'"
     ]
    }
   ],
   "source": [
    "annotations_dir = \"NEW_DATA/annotations\"  # <-- change this\n",
    "images_dir = \"NEW_DATA/images\"            # <-- change this\n",
    "\n",
    "dataset = ClimbingHoldDataset(\n",
    "    annotations_dir=annotations_dir,\n",
    "    images_dir=images_dir,\n",
    "    output_size=(IMG_SIZE, IMG_SIZE),\n",
    ")\n",
    "\n",
    "dataset.transform = eval_transform  # use eval transform\n",
    "\n",
    "print(\"Total samples in NEW dataset:\", len(dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209929c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoHeadResNet50(nn.Module):\n",
    "    def __init__(self, num_types=7, num_orientations=5, pretrained=False, dropout=0.3):\n",
    "        super().__init__()\n",
    "        weights = ResNet50_Weights.IMAGENET1K_V1 if pretrained else None\n",
    "        backbone = models.resnet50(weights=weights)\n",
    "\n",
    "        in_features = backbone.fc.in_features\n",
    "        backbone.fc = nn.Identity()\n",
    "        self.backbone = backbone\n",
    "\n",
    "        self.type_head = nn.Sequential(\n",
    "            nn.Linear(in_features, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(512, num_types),\n",
    "        )\n",
    "\n",
    "        self.orientation_head = nn.Sequential(\n",
    "            nn.Linear(in_features, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(256, num_orientations),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        feats = self.backbone(x)\n",
    "        type_logits = self.type_head(feats)\n",
    "        orientation_logits = self.orientation_head(feats)\n",
    "        return type_logits, orientation_logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f907e4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_types = 7\n",
    "num_orients = 5\n",
    "\n",
    "model = TwoHeadResNet50(\n",
    "    num_types=num_types,\n",
    "    num_orientations=num_orients,\n",
    "    pretrained=False,  # weights come from checkpoint\n",
    "    dropout=0.3,\n",
    ").to(device)\n",
    "\n",
    "state_dict = torch.load(\"best_hold_model.pth\", map_location=device)\n",
    "model.load_state_dict(state_dict)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(\"Loaded trained model from best_hold_model.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0f70e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "all_pred_types = []\n",
    "all_pred_orients = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in loader:\n",
    "        imgs = batch[\"image\"].to(device)  # from __getitem__\n",
    "        out_type, out_orient = model(imgs)\n",
    "\n",
    "        pred_type_batch = out_type.argmax(1).cpu().numpy()\n",
    "        pred_orient_batch = out_orient.argmax(1).cpu().numpy()\n",
    "\n",
    "        all_pred_types.append(pred_type_batch)\n",
    "        all_pred_orients.append(pred_orient_batch)\n",
    "\n",
    "pred_type_idx = np.concatenate(all_pred_types)   # shape (N,)\n",
    "pred_orient_idx = np.concatenate(all_pred_orients)\n",
    "\n",
    "print(\"Predicted types shape   :\", pred_type_idx.shape)\n",
    "print(\"Predicted orients shape :\", pred_orient_idx.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b74dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for hold, t, o in zip(dataset.holds, pred_type_idx, pred_orient_idx):\n",
    "    hold[\"pred_type_idx\"] = int(t)\n",
    "    hold[\"pred_orient_idx\"] = int(o)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7630b699",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_new = pd.DataFrame(dataset.holds)\n",
    "df_new.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c467c90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.to_csv(\"new_dataset_with_predictions.csv\", index=False)\n",
    "print(\"Saved new_dataset_with_predictions.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
